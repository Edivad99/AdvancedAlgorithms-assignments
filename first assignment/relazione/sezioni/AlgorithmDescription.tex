\section{Description of the algorithms}

\subsection{Data structures}
In the following subsections, we will illustrate the main data structures used as a common foundation for all the 
three algorithms for calculating a \textbf{Minimum Spannin Tree}.

\subsubsection{Data structure for a vertex}
The data structure for a vertex was implemented as follows:
\begin{enumerate}
    \item \verb|Name| of the vertex;
    \item \verb|Key| is a kind of weight of a vertex;
    \item \verb|Parent| indicates the parent of a vertex;
    \item \verb|VerticesAdjacent| is a set which contains all adjacent nodes; 
    \item \verb|Visited| that indicates if a node has already been visited or not.
\end{enumerate}
\noindent
The implemented methods were:
\begin{enumerate}
    \item \verb|IsVisited|: returns the value of \verb|Visited|;
    \item \verb|SetVisited|: sets the value of \verb|Visited| to true or false as indicated by the parameter;
    \item \verb|AddAdjacentVertices|: adds an adjacent vertex to the \verb|VerticesAdjacent| of a node;
    \item \verb|RemoveAdjecentVertices|: removes an adjacent vertex to the \verb|VerticesAdjacent|;
    \item \verb|Equals|: checks if two vetrices have the same name.
\end{enumerate}
These operations have been implemented in order to have a constant computational complexity $\mathcal{O}(1)$.

\subsubsection{Data structure for an edge}
The data structure for an edge is the following one:
\begin{enumerate}
    \item \verb|U| represents one end of the edge;
    \item \verb|V| represents one end of the edge;
    \item \verb|Weight| indicates the weight of the edge.
    \end{enumerate}
\noindent
The implemented methods were:
\begin{enumerate}
    \item \verb|CompareTo|: compares the weight of two different edges and returns an indication of which one is the greater;
    \item \verb|Equals|: checks if two edges have the same ends and weight.
\end{enumerate}
These operations have been implemented in order to have a constant computational complexity $\mathcal{O}(1)$.

\subsubsection{Data structure for a graph}
The data structure for a graph was implemented as follows:
\begin{enumerate}
    \item \verb|V| is a dictionary of vertices, with key the name of the vertex;
    \item \verb|E| is a dictionary of edges, with key the tuple of the names of the two vertices.
    \end{enumerate}
\noindent
The implemented methods were:
\begin{enumerate}
    \item \verb|AddVertex|: adds a vertex to the graph;
    \item \verb|AddEdge|: adds an edge to the graph;
    \item \verb|RemoveEdge|: removes an edge from the dictionary \verb|E|;
    \item \verb|GetWeight|: returns the weight of an edge given two vertices;
    \item \verb|LoadFromFileAsync|: creates the graph starting from the selected file.
\end{enumerate}
These operations (except \verb|LoadFromFileAsync|) have been implemented in order to have a constant computational complexity $\mathcal{O}(1)$.

\subsection{Prim}
\subsubsection{Introduction}
The basic version of Prim's algorithm has a computational complexity of $\mathcal{O}(mn)$.
The initial intuition is to start with a random vertex \verb|s| and choose, for each iteration, the edge 
that connects one node in the \textit{MST} tree with the lowest possible weight to a new vertex.\\
The algorithm can be optimized by using the correct data structure.
In fact, by using the \textit{Heap}, a data structure that implements a priority queue, we can compute the 
minimum weight in logarithmic time and the complexity becomes $\mathcal{O}(mlog(n))$. \\

\noindent
The pseudo-code of Prim's algorithm using \textit{Heap} is illustrated below:
\begin{verbatim}
  Prim(G,s)
    for each u in V do
      key[u] <- +inf
      parent[u] <- nil
    key[s] <- 0
    Q <- V
    while Q not empty do
      u <- extractMin(Q)
      for each v adjacent to u do
        if v in Q and w(u,v) < key[v] then
          key[v] <- w(u,v)
          parent[v] <- u
\end{verbatim}

\subsubsection{Our implementation}
For the heap structure we used a \textbf{PriorityQueue} that was recently added in .NET 6.0.
The PriorityQueue creates an association between the Vertex object that we pass to it and the weight that vertex has.\\
Our implementation is slightly different from the original one. In fact, instead of filling the PriorityQueue with 
all the vertices, we started by inserting only the starting node with the weight of zero.\\
Then, in the while, we extracting the minimum vertex and we check if the vertex was already visited.
If it was visited we skip it and we extract another one.
Otherwise we mark it as visited, and we starting to check all its adjacent vertices.
In the if condition, we check that the adjacent vertices was not visited yet and if the conditions succeeds 
we adding the vertex to the PriorityQueue.
So the code become:
\begin{verbatim}
  Prim(G,s)
    for each u in V do
      key[u] <- +inf
      parent[u] <- nil
    key[s] <- 0
    Q <- {s}
    while Q not empty do
      u <- extractMin(Q)
      if not visited[u] do
        visited[u] <- true
        for each v adjacent to u do
          if not visited[v] and w(u,v) < key[v] then
            key[v] <- w(u,v)
            parent[v] <- u
            Q <- Q U {v}
\end{verbatim}

\subsection{Kruskal naive}
\subsubsection{Introduction}
The naive version of Kruskal has a time complexity of $\mathcal{O}(mn)$. The main idea is to sort the edges by 
their weight in increasing order.
Once sorted, for each edge we checking if by adding it we create a cycle. If it creates a cycle it wont be 
added, otherwise it will be part of the \textit{MST}. By sorting the edges by their weight, we are sure that the sum of the
edges will be minimum. \\

\noindent
The pseudo-code of naive Kruskal's algorithm is as follows:
\begin{verbatim}
  Kruskal-Naive(G)
    A = empty_set
    sort the edges of G.E into increasing order by weight
    for each edge e in G.E, taken in increasing order by weight
      if A U {e} is acyclic
        A = A U {e}
    return A
\end{verbatim}
\subsubsection{Our implementation}
In our implementation, we used \verb|sort|, a method provided directly by C\#, which has a computational complexity equal to $\mathcal{O}(n log(n))$ to order the edges in a non-decreasing order of weight.
Moreover, instead of using A as set we decided to use our implementation of the graph $G$.
The cyclicality check is implemented with a modified version of \textit{Depth First Search}(\textit{DFS}).\\
The first optimization implemented is in the \verb|IsAcyclic| method, where we check if, first of all, the edge that we want 
to insert is a self loop. Next we check if both vertices are already present in the dictionary of the vertices.
If one of them is not present we are sure that the edge that we want insert won't create a cycle.
This approach will let us reduce the calls to DFS in the first phase of the exploration.
If both vertices are present in the dictionary, we need to call DFS to check if we'll create a cycle by adding the edge.\\
The second optimization concerns DFS: the method that is implemented it does not reflect the original version 
of the algorithm. Our intent is to find, if it exists, a path that joins $u$ with $v$. If these vertices are found 
in two distinct connected components, means that adding the edge $(u, v)$ to the graph $A$ will not introduce a cycle. 
Starting DFS from vertex $u$ on the graph $A$, we want to check if there is a path that leads to $v$.
If both vertices are in the same connected component, then such a path will be found, otherwise such a path will 
not be found because it doesn't exist. Hence, DFS will only run on the connected component of $u$ and not on all 
vertices of the graph. This version allows us to save the visit of the whole graph and its computational complexity 
becomes $\mathcal{O}(m)$. To conclude, these optimizations allow us to avoid exessive calls to DFS and avoid visiting the entire
graph even when a path from $u$ and $v$ does not exist. So, the computational complexity of the Kruskal Naive algorithm remains equal to $\mathcal{O}(mn)$.

\subsection{Kruskal with Union-Find}

\subsubsection{Introduction}
The naive version of Kruskal's algorithm performs repeatedly the cycle check operation on the graph. 
This constant control is responsible fot the computational complexity of the algorithm, so, in order to speed up the 
naive Kruskal's algorithm, we can use a new data structure called: \textbf{Disjoint Set} or \textbf{Union Find}.\\

\noindent
The data structure for representing the disjoint sets was implemented as a tree. Initially, each vertex is represented as a single node tree, whose parent is itself.
As we go to visit the edges that makes the graph $G$, we check if the two vertices of the selected edge belong to the same tree or not.
If they belong to the tree, it means that they have the same root node, otherwise the root node will be different.
Neither the order of the nodes saved in the tree nor which node is selected to be a root of the tree is important for the implementation of this algorithm.
If the vertices of the edge $(u, v)$ belong to two different trees, then we perform an \textit{union} of these two trees. In order to implement this operation in the data structure, we taking into account also the size of each tree;
each node stores its size which represents the number of nodes that makes its tree.
When two different trees needs to be joined together, the tree with the smallest size becomes a child of the other; if the two tree have the same size then the choice is not so relevant, 
it's possible to make the first tree a child of the second or viceversa.
Thanks to this new approach we are able to reduce the time complexity to $\mathcal{O}(m log(n))$. \\

\noindent
The pseudo-code of Kruskal's algorithm using Union-Find is illustrated below:
\begin{verbatim}
  Kruskal-Union-Find(G)
    A = empty_set
    U = union_find_ds
    for each vertex v in G.V
      U.MakeSet(v)
    sort the edges of G.E into increasing order by weight
    for each edge (v, w) in G.E, taken in increasing order by weight
      if U.FindSet(v) != U.FindSet(w)
        A = A U {(v, w)}
        U.Union(v, w)
    return A
\end{verbatim}
\noindent
The check for the cyclicality of the graph is replaced with the verification whether the two vertices of edge $(v, w)$ 
belong to two distinct trees or not. If the two vertices have the root node in common it means that a path from $v$ to $w$ already exists 
and thus adding the edge $(v, w)$ it would create a cycle.
If, on the other hand, the two vertices belong to two different trees, then the introduction of such an arch will 
not create a cycle in the graph.
Then, the edge $(v, w)$ will be added to $A$ and the two trees will be joined based on their \textit{size}.

\subsubsection{Our implementation}
Since in C\# the UF data structure is not present, we implemented it.\\
The principal methods available in the Union Find data structure are:
\begin{enumerate}
    \item \verb|MakeSet(x)|: where we pass the data that we want to insert in the data structure and we return 
        if the data was inserted or not. The time complexity of this method is $\mathcal{O}(1)$;
    \item \verb|FindSet(x)|: returns the root node of the tree to which the node that has value $x$ belongs.
        The time complexity of this method is $\mathcal{O}(log(n))$;
    \item \verb|Union(x, y)|: given the values of the two nodes, we determine the parent node for each node and 
        if they belong to different tree we merge them in base at their size. The time complexity of this 
        method is $\mathcal{O}(log(n))$.
\end{enumerate}
\noindent
Using our implementation of the Union-find data structure and analyzing the times in which the above mentioned methods are called, 
we can say that the final computational complexity of our Kruskal Union-Find algorithm is $\mathcal{O}(m log(n))$
